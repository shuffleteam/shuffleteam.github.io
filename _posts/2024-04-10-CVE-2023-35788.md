---
layout: post
title: "Root linux via an off-by-one vulnerability(CVE-2023-35788)"
author: shuffle
description:
categories: [ research ]
tags: [ cve-2023-35788, Linux, Kernel, exploit ]
comments: true
featured: true
image: assets/images/2024-04-15/1.png
---



## 0x00 Introduction

The vulnerability is an off-by-one flaw located within the Linux network scheduler subsystem (NET_SCHED). It allows bypassing all protective measures (though a new namespace needs to be created) and enables privilege escalation on Linux systems.

NET_SCHED in Linux is the Network Scheduler subsystem, which is responsible for traffic scheduling and management in the network stack. It provides a range of scheduling algorithms and queue management mechanisms, including priority queuing, token bucket scheduling, and random early detection, enabling users to flexibly control and optimize network traffic as needed.

This vulnerability is not a new one. It was already patched in Linux 6.4-rc5 (June 2023) by [me](https://twitter.com/HBh25Y). However, due to its interesting nature, involving out-of-bounds access within a structure, it does not cause a kernel crash when triggered. Moreover, its exploit stability is quite high. So, I have decided to outline the exploitation approach for this vulnerability.

Since NET_SCHED is a well-known module that has experienced several vulnerabilities (you can check KCTF), and also because I'm a bit lazyðŸ˜Œ. This article will focus solely on the vulnerability itself.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/meme1.jpeg">
</div>

## 0x01 Vulnerability Analysis

The vulnerability occurs in the Flower classifier of NET_SCHED.

The core function containing the vulnerability is `fl_set_geneve_opt()`. Below is the code snippet provided:

```c
static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,
			     int depth, int option_len,
			     struct netlink_ext_ack *extack)
{
	struct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];
	struct nlattr *class = NULL, *type = NULL, *data = NULL;
	struct geneve_opt *opt;
	int err, data_len = 0;

	if (option_len > sizeof(struct geneve_opt))
		data_len = option_len - sizeof(struct geneve_opt);

	opt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];	<--- [1]
	memset(opt, 0xff, option_len);
	opt->length = data_len / 4;
	opt->r1 = 0;
	opt->r2 = 0;
	opt->r3 = 0;

	/* If no mask has been prodived we assume an exact match. */
	if (!depth)
		return sizeof(struct geneve_opt) + data_len;

	if (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {
		NL_SET_ERR_MSG(extack, "Non-geneve option type for mask");
		return -EINVAL;
	}

	err = nla_parse_nested_deprecated(tb,
					  TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,
					  nla, geneve_opt_policy, extack);					<--- [2]
	if (err < 0)
		return err;

	/* We are not allowed to omit any of CLASS, TYPE or DATA
	 * fields from the key.
	 */
	if (!option_len &&
	    (!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||
	     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||
	     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {
		NL_SET_ERR_MSG(extack, "Missing tunnel key geneve option class, type or data");
		return -EINVAL;
	}

	/* Omitting any of CLASS, TYPE or DATA fields is allowed
	 * for the mask.
	 */
	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {
		int new_len = key->enc_opts.len;								<--- [3]

		data = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];
		data_len = nla_len(data);
		if (data_len < 4) {												<--- [4]
			NL_SET_ERR_MSG(extack, "Tunnel key geneve option data is less than 4 bytes long");
			return -ERANGE;
		}
		if (data_len % 4) {												<--- [5]
			NL_SET_ERR_MSG(extack, "Tunnel key geneve option data is not a multiple of 4 bytes long");
			return -ERANGE;
		}

		new_len += sizeof(struct geneve_opt) + data_len;
		BUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);
		if (new_len > FLOW_DIS_TUN_OPTS_MAX) {							<--- [6]
			NL_SET_ERR_MSG(extack, "Tunnel options exceeds max size");
			return -ERANGE;
		}
		opt->length = data_len / 4;
		memcpy(opt->opt_data, nla_data(data), data_len);				<--- [7]
	}

	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {
		class = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];
		opt->opt_class = nla_get_be16(class);
	}

	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {
		type = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];
		opt->type = nla_get_u8(type);
	}

	return sizeof(struct geneve_opt) + data_len;
}
```

At position [1], the function `fl_set_geneve_opt()` directly uses `key->enc_opts.len` as an array parameter, retrieving the `opt` structure from the `key->enc_opts.data` array, and setting the data of length `option_len` to 0xff. Additionally, some fields of the `opt` structure are initialized. Upon examining the `struct geneve_opt` structure of `opt`, it is evident that initializing the `geneve_opt` structure requires exactly 4 bytes.

```c
struct geneve_opt {
	__be16	opt_class;
	u8	type;
#ifdef __LITTLE_ENDIAN_BITFIELD
	u8	length:5;
	u8	r3:1;
	u8	r2:1;
	u8	r1:1;
#else
	u8	r1:1;
	u8	r2:1;
	u8	r3:1;
	u8	length:5;
#endif
	u8	opt_data[];
};
```

Continuing to position [2], the function parses the user-provided packet data into the `tb` array. Additionally, due to the set `geneve_opt_policy`, if data of type TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA is passed in (parsed at position [3]), it must not exceed 128 bytes.

```c
static const struct nla_policy
geneve_opt_policy[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1] = {
	[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]      = { .type = NLA_U16 },
	[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]       = { .type = NLA_U8 },
	[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]       = { .type = NLA_BINARY,
						       .len = 128 },
};
```

Here we arrive at position [3]. We can observe that the variable `key->enc_opts.len`, used for the array, is assigned to `new_len`. Since `tb` is user-supplied data, positions [4] and [5] are used to ensure that the data we pass in is divisible by 4 and greater than the packet header, i.e., the `struct geneve_opt` structure. Then, at position [6], the code checks whether the length of `key->enc_opts.len` plus the length of our newly passed packet data is less than 255, which is the size of the `key->enc_opts.data` array. After this series of validations, the data is finally copied to the `opt_data` field of `geneve_opt` using `memcpy` at position [7]. In the end, `fl_set_geneve_opt` will return the length of the new packet header plus the length of the data.

Now, let's see how `key->enc_opts.len` is assigned.

```c
static int fl_set_enc_opt(struct nlattr **tb, struct fl_flow_key *key,
			  struct fl_flow_key *mask,
			  struct netlink_ext_ack *extack)
{
...
	if (tb[TCA_FLOWER_KEY_ENC_OPTS_MASK]) {
		err = nla_validate_nested_deprecated(tb[TCA_FLOWER_KEY_ENC_OPTS_MASK],
						     TCA_FLOWER_KEY_ENC_OPTS_MAX,
						     enc_opts_policy, extack);
		if (err)
			return err;

		nla_opt_msk = nla_data(tb[TCA_FLOWER_KEY_ENC_OPTS_MASK]);
		msk_depth = nla_len(tb[TCA_FLOWER_KEY_ENC_OPTS_MASK]);
		if (!nla_ok(nla_opt_msk, msk_depth)) {
			NL_SET_ERR_MSG(extack, "Invalid nested attribute for masks");
			return -EINVAL;
		}
	}

	nla_for_each_attr(nla_opt_key, nla_enc_key,
			  nla_len(tb[TCA_FLOWER_KEY_ENC_OPTS]), key_depth) {
		switch (nla_type(nla_opt_key)) {
		case TCA_FLOWER_KEY_ENC_OPTS_GENEVE:
			if (key->enc_opts.dst_opt_type &&
			    key->enc_opts.dst_opt_type != TUNNEL_GENEVE_OPT) {
				NL_SET_ERR_MSG(extack, "Duplicate type for geneve options");
				return -EINVAL;
			}
			option_len = 0;
			key->enc_opts.dst_opt_type = TUNNEL_GENEVE_OPT;
			option_len = fl_set_geneve_opt(nla_opt_key, key,	
						       key_depth, option_len,
						       extack);
			if (option_len < 0)
				return option_len;

			key->enc_opts.len += option_len;						<--- [8]
			/* At the same time we need to parse through the mask
			 * in order to verify exact and mask attribute lengths.
			 */
			mask->enc_opts.dst_opt_type = TUNNEL_GENEVE_OPT;
			option_len = fl_set_geneve_opt(nla_opt_msk, mask,
						       msk_depth, option_len,
						       extack);
			if (option_len < 0)
				return option_len;

			mask->enc_opts.len += option_len;						<--- [9]
			if (key->enc_opts.len != mask->enc_opts.len) {			<--- [10]
				NL_SET_ERR_MSG(extack, "Key and mask miss aligned");
				return -EINVAL;
			}
			break;
...
		if (!msk_depth)
			continue;

		if (!nla_ok(nla_opt_msk, msk_depth)) {
			NL_SET_ERR_MSG(extack, "A mask attribute is invalid");
			return -EINVAL;
		}
		nla_opt_msk = nla_next(nla_opt_msk, &msk_depth);
	}
}
```

We observe that the higher-level function of `fl_set_geneve_opt` is `fl_set_enc_opt`. `fl_set_enc_opt` is responsible for determining the possibility of passing in multiple data packets. Therefore, it iterates internally to parse each packet of `TCA_FLOWER_KEY_ENC_OPTS_GENEVE` type. These packets are categorized into key and mask types, with the requirement that there must be a key before a mask, and the lengths of the key and mask must be equal (checked at position [10]). The `enc_opts.len` we are concerned with represents the cumulative length of the data packets already passed in.

So far, it appears there isn't an issue because even if there's an array overflow at position [1], the lengths of `key->enc_opts.len` and the subsequent data packets will be checked, and the process will fail due to failed validation. 

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/meme2.png">
</div>

However, let's take a closer look at the structure containing this potentially overflowing array.

```c
struct flow_dissector_key_enc_opts {
	u8 data[FLOW_DIS_TUN_OPTS_MAX];	/* Using IP_TUNNEL_OPTS_MAX is desired
					 * here but seems difficult to #include
					 */
	u8 len;
	__be16 dst_opt_type;
};
```

The `data` array is of length 255, followed immediately by `len`, which is of type `u8`, occupying just one byte with a maximum value of 255. However, the issue arises here. Suppose we pass in two or more data packets with a cumulative total size of 252. Due to the initialization of a 4-byte `struct geneve_opt` header directly at position [1], an off-by-one error occurs. Fortunately, this off-by-one error precisely overwrites the `len` field of the `flow_dissector_key_enc_opts` structure. Thus, relying on this off-by-one error, we can access position `key->enc_opts.data[252]`. However, `key->enc_opts.len` is overwritten with a value less than 128 (calculated as 4 + package length/4 + package length). This means `key->enc_opts.len = opt->length = data_len / 4`. Consequently, we can bypass the checks at positions [4], [5], and [6], resulting in an out-of-bounds write of up to 128 bytes.

## 0x02 Exploit

Now that we have a maximum of 128 bytes for the out-of-bounds write, since the overflow occurs in a nested structure, let's first trace the function call chain of `fl_set_geneve_opt`.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/1.png">
</div>


As shown in the above diagram, the top-level functions are `tc_new_tfilter` and `tc_ctl_chain`.

```c
static int __init tc_filter_init(void)
{
	int err;

	tc_filter_wq = alloc_ordered_workqueue("tc_filter_workqueue", 0);
	if (!tc_filter_wq)
		return -ENOMEM;

	err = register_pernet_subsys(&tcf_net_ops);
	if (err)
		goto err_register_pernet_subsys;

	rtnl_register(PF_UNSPEC, RTM_NEWTFILTER, tc_new_tfilter, NULL,
		      RTNL_FLAG_DOIT_UNLOCKED);
	rtnl_register(PF_UNSPEC, RTM_DELTFILTER, tc_del_tfilter, NULL,
		      RTNL_FLAG_DOIT_UNLOCKED);
	rtnl_register(PF_UNSPEC, RTM_GETTFILTER, tc_get_tfilter,
		      tc_dump_tfilter, RTNL_FLAG_DOIT_UNLOCKED);
	rtnl_register(PF_UNSPEC, RTM_NEWCHAIN, tc_ctl_chain, NULL, 0);
	rtnl_register(PF_UNSPEC, RTM_DELCHAIN, tc_ctl_chain, NULL, 0);
	rtnl_register(PF_UNSPEC, RTM_GETCHAIN, tc_ctl_chain,
		      tc_dump_chain, 0);

	return 0;

err_register_pernet_subsys:
	destroy_workqueue(tc_filter_wq);
	return err;
}
```


Here, `tc_new_tfilter` and `tc_ctl_chain` are respectively called when the user passes in types such as RTM_NEWTFILTER and RTM_NEWCHAIN via Netlink.

The intermediate layer's virtual functions are located in `cls_fl_ops`. Positions [11] and [12] are where we can trigger the vulnerable function.

```c
static struct tcf_proto_ops cls_fl_ops __read_mostly = {
	.kind		= "flower",
	.classify	= fl_classify,
	.init		= fl_init,
	.destroy	= fl_destroy,
	.get		= fl_get,
	.put		= fl_put,
	.change		= fl_change,				<--- [11]
	.delete		= fl_delete,
	.delete_empty	= fl_delete_empty,
	.walk		= fl_walk,
	.reoffload	= fl_reoffload,
	.hw_add		= fl_hw_add,
	.hw_del		= fl_hw_del,
	.dump		= fl_dump,
	.terse_dump	= fl_terse_dump,
	.bind_class	= fl_bind_class,
	.tmplt_create	= fl_tmplt_create,		<--- [12]
	.tmplt_destroy	= fl_tmplt_destroy,
	.tmplt_dump	= fl_tmplt_dump,
	.owner		= THIS_MODULE,
	.flags		= TCF_PROTO_OPS_DOIT_UNLOCKED,
};
```

Once we've identified the call chain, we can follow these two chains to find the objects we can overflow. The two directly overflowable objects are as follows:

```c
struct fl_flow_tmplt {
	struct fl_flow_key dummy_key;      <--- [13]
	struct fl_flow_key mask;		   <--- [14]
	struct flow_dissector dissector;
	struct tcf_chain *chain;
};

struct cls_fl_filter {
	struct fl_flow_mask *mask;
	struct rhash_head ht_node;
	struct fl_flow_key mkey;
	struct tcf_exts exts;
	struct tcf_result res;
	struct fl_flow_key key;				 <--- [15]
	struct list_head list;
	struct list_head hw_list;
	u32 handle;
	u32 flags;
	u32 in_hw_count;
	struct rcu_work rwork;
	struct net_device *hw_dev;
	/* Flower classifier is unlocked, which means that its reference counter
	 * can be changed concurrently without any kind of external
	 * synchronization. Use atomic reference counter to be concurrency-safe.
	 */
	refcount_t refcnt;
	bool deleted;
};
```

We focus on the `fl_flow_tmplt` structure. Since we can overflow 128 bytes and overwrite the object pointer of `chain`, modifying `chain` allows us to perform interesting fabrications when `fl_flow_tmplt` calls this object. Additionally, we find that the `cls_fl_ops` virtual functions include a `fl_tmplt_destroy` function.

```c
static void fl_tmplt_destroy(void *tmplt_priv)
{
	struct fl_flow_tmplt *tmplt = tmplt_priv;

	fl_hw_destroy_tmplt(tmplt->chain, tmplt);							<--- [16]
	kfree(tmplt);
}

static void fl_hw_destroy_tmplt(struct tcf_chain *chain,
				struct fl_flow_tmplt *tmplt)
{
	struct flow_cls_offload cls_flower = {};
	struct tcf_block *block = chain->block;								<--- [17]

	cls_flower.common.chain_index = chain->index;
	cls_flower.command = FLOW_CLS_TMPLT_DESTROY;
	cls_flower.cookie = (unsigned long) tmplt;

	tc_setup_cb_call(block, TC_SETUP_CLSFLOWER, &cls_flower, false, true);
}

int tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
		     void *type_data, bool err_stop, bool rtnl_held)
{
	bool take_rtnl = READ_ONCE(block->lockeddevcnt) && !rtnl_held;
	int ok_count;

retry:
	if (take_rtnl)
		rtnl_lock();
	down_read(&block->cb_lock);
	/* Need to obtain rtnl lock if block is bound to devs that require it.
	 * In block bind code cb_lock is obtained while holding rtnl, so we must
	 * obtain the locks in same order here.
	 */
	if (!rtnl_held && !take_rtnl && block->lockeddevcnt) {
		up_read(&block->cb_lock);
		take_rtnl = true;
		goto retry;
	}

	ok_count = __tc_setup_cb_call(block, type, type_data, err_stop);	<--- [18]

	up_read(&block->cb_lock);
	if (take_rtnl)
		rtnl_unlock();
	return ok_count;
}

static int
__tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
		   void *type_data, bool err_stop)
{
	struct flow_block_cb *block_cb;
	int ok_count = 0;
	int err;

	list_for_each_entry(block_cb, &block->flow_block.cb_list, list) {
		err = block_cb->cb(type, type_data, block_cb->cb_priv);			<--- [19] 
		if (err) {
			if (err_stop)
				return err;
		} else {
			ok_count++;
		}
	}
	return ok_count;
}
```

By tracing the `fl_tmplt_destroy` function, we can observe that at position [16], we can control `tmplt->chain`. Therefore, at positions [17] and [18], we can control `block`. At position [19], we have a suitable pointer, enabling control flow hijacking.

At this point, our strategy is clear. We need to leak a controllable heap address, while simultaneously bypassing KASLR to obtain usable function pointers. Both of these tasks require out-of-bounds reads.

### Transforming the off-by-one vulnerability into an out-of-bounds read vulnerability

We need out-of-bounds reads, and conveniently, from `cls_fl_ops`, we can see two functions that transfer data from the kernel to user space: `fl_dump` and `fl_tmplt_dump`. These correspond to the two vulnerable functions we just discussed.

```c
static struct tcf_proto_ops cls_fl_ops __read_mostly = {
	.kind		= "flower",
...
	.change		= fl_change,				<--- [11]
	.delete		= fl_delete,
...
	.dump		= fl_dump,
	.terse_dump	= fl_terse_dump,
	.bind_class	= fl_bind_class,
	.tmplt_create	= fl_tmplt_create,		<--- [12]
	.tmplt_destroy	= fl_tmplt_destroy,
	.tmplt_dump	= fl_tmplt_dump,
	.owner		= THIS_MODULE,
	.flags		= TCF_PROTO_OPS_DOIT_UNLOCKED,
};
```

Following the call chains of these two functions, it's logical to use `fl_dump_key_geneve_opt` to retrieve the data we previously passed in.

```
fl_dump_key -> fl_dump_key_enc_opt -> fl_dump_key_options->fl_dump_key_geneve_opt
```

Let's take a closer look at `fl_dump_key_geneve_opt`.

```c
static int fl_dump_key_geneve_opt(struct sk_buff *skb,
				  struct flow_dissector_key_enc_opts *enc_opts)
{
	struct geneve_opt *opt;
	struct nlattr *nest;
	int opt_off = 0;

	nest = nla_nest_start_noflag(skb, TCA_FLOWER_KEY_ENC_OPTS_GENEVE);
	if (!nest)
		goto nla_put_failure;

	while (enc_opts->len > opt_off) {								<--- [20]
		opt = (struct geneve_opt *)&enc_opts->data[opt_off];		<--- [21]

		if (nla_put_be16(skb, TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS,
				 opt->opt_class))
			goto nla_put_failure;
		if (nla_put_u8(skb, TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE,
			       opt->type))
			goto nla_put_failure;
		if (nla_put(skb, TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA,
			    opt->length * 4, opt->opt_data))
			goto nla_put_failure;

		opt_off += sizeof(struct geneve_opt) + opt->length * 4;		<--- [22]
	}
	nla_nest_end(skb, nest);
	return 0;

nla_put_failure:
	nla_nest_cancel(skb, nest);
	return -EMSGSIZE;
}
```

`fl_dump_key_geneve_opt` simply utilizes the previously controlled `enc_opts->len` to control the length, and copies data based on each `opt->length * 4`, sending it to user space.

Let's return to `fl_set_geneve_opt`.

```c
static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,
			     int depth, int option_len,
			     struct netlink_ext_ack *extack)
{
	struct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];
	struct nlattr *class = NULL, *type = NULL, *data = NULL;
	struct geneve_opt *opt;
	int err, data_len = 0;

	if (option_len > sizeof(struct geneve_opt))
		data_len = option_len - sizeof(struct geneve_opt);

	opt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];	<--- [1]
	memset(opt, 0xff, option_len);
	opt->length = data_len / 4;
	opt->r1 = 0;
	opt->r2 = 0;
	opt->r3 = 0;

	/* If no mask has been prodived we assume an exact match. */
	if (!depth)
		return sizeof(struct geneve_opt) + data_len;

	if (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {
		NL_SET_ERR_MSG(extack, "Non-geneve option type for mask");
		return -EINVAL;
	}

	err = nla_parse_nested_deprecated(tb,
					  TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,
					  nla, geneve_opt_policy, extack);					<--- [2]
	if (err < 0)
		return err;

	/* We are not allowed to omit any of CLASS, TYPE or DATA
	 * fields from the key.
	 */
	if (!option_len &&
	    (!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||
	     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||
	     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {
		NL_SET_ERR_MSG(extack, "Missing tunnel key geneve option class, type or data");
		return -EINVAL;
	}

	/* Omitting any of CLASS, TYPE or DATA fields is allowed
	 * for the mask.
	 */
	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {
		int new_len = key->enc_opts.len;								<--- [3]

		data = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];
		data_len = nla_len(data);
		if (data_len < 4) {												<--- [4]
			NL_SET_ERR_MSG(extack, "Tunnel key geneve option data is less than 4 bytes long");
			return -ERANGE;
		}
		if (data_len % 4) {												<--- [5]
			NL_SET_ERR_MSG(extack, "Tunnel key geneve option data is not a multiple of 4 bytes long");
			return -ERANGE;
		}

		new_len += sizeof(struct geneve_opt) + data_len;
		BUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);
		if (new_len > FLOW_DIS_TUN_OPTS_MAX) {							<--- [6]
			NL_SET_ERR_MSG(extack, "Tunnel options exceeds max size");
			return -ERANGE;
		}
		opt->length = data_len / 4;
		memcpy(opt->opt_data, nla_data(data), data_len);				<--- [7]
	}

	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {
		class = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];
		opt->opt_class = nla_get_be16(class);
	}

	if (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {
		type = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];
		opt->type = nla_get_u8(type);
	}

	return sizeof(struct geneve_opt) + data_len;
}
```

Triggering the off-by-one requires passing in two or more data packets, with their combined lengths totaling exactly 252 bytes. Using the 4-byte initialization to overwrite `key->enc_opts.len` allows us to bypass checks [4], [5], and [6]. Due to the checks, the value of `key->enc_opts.len` can never exceed 255 after a successful `memcpy`. However, if we need to perform out-of-bounds reads using `fl_dump_key_geneve_opt`, we must do so under the condition specified at [20], which appears somewhat tricky.

However, if we break out of the cycle of overwriting `key->enc_opts.len` to bypass detection and perform out-of-bounds access, we should consider another functionality of `key->enc_opts.len`: locating the position of the array for incoming data packets. The length of each packet header we pass in is also in this array, upon which `fl_dump_key_geneve_opt` relies for each data read. Since we've reset `key->enc_opts.len` to a value less than 128, we can refill the array and modify `opt->length`.

Let's examine this scenario.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/2.png">
</div>

First, we pass in two data packets, as shown in the diagram above, with a total length of exactly 252 bytes, leaving 3 bytes remaining in the array. Then, we pass in another data packet, causing an overflow of 1 byte into `key->enc_opts.len`.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/3.png">
</div>

At this point, we pass in another data packet. Referring back to the `geneve_opt` structure, we know that the `length`, `r3`, `r2`, and `r1` fields together form the `key->enc_opts.len` byte. However, the `length` field will be assigned the value of the data packet length divided by 4. Therefore, if we overflow with a data packet of 96 bytes in length, `key->enc_opts.len` will precisely points to the end of the orange section.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/4.png">
</div>

Finally, we pass in another data packet, continuously overwriting until the last 252 bytes. With this data packet, we modify the length of the header of the first 120-byte blue packet passed in initially, making it point to a fake head, and forge the fake head in the last four bytes.

Due to the nature of the `fl_dump_key_geneve_opt` function, as our fake head falls within the length range of `key->enc_opts.len`, it will read the length of the fake head's data packet, resulting in an out-of-bounds read of 128 bytes.

With the out-of-bounds read, the `chain` in the `struct fl_flow_tmplt` structure serves as our target for heap leakage. In addition to the heap address, we also need function addresses to bypass KASLR protection.

```c
static int fl_change(struct net *net, struct sk_buff *in_skb,
		     struct tcf_proto *tp, unsigned long base,
		     u32 handle, struct nlattr **tca,
		     void **arg, u32 flags,
		     struct netlink_ext_ack *extack)
{
...
	err = fl_set_parms(net, tp, fnew, mask, base, tb, tca[TCA_RATE],
			   tp->chain->tmplt_priv, flags, extack);
	if (err)
		goto errout;

	err = fl_check_assign_mask(head, fnew, fold, mask);			<--- [23]
	if (err)
		goto errout;
...
}
```

Examining the function call flow of `fl_set_key`, at position [23], our passed-in key and mask are processed, and the result is stored in `f->mkey`. Unlike with the out-of-bounds write, we now have an additional structure, `struct fl_flow_mask`, that we can perform out-of-bounds reads on.

```c
struct fl_flow_mask {
	struct fl_flow_key key;
	struct fl_flow_mask_range range;
	u32 flags;
	struct rhash_head ht_node;
	struct rhashtable ht;
	struct rhashtable_params filter_ht_params;
	struct flow_dissector dissector;
	struct list_head filters;
	struct rcu_work rwork;
	struct list_head list;
	refcount_t refcnt;
};

struct rhashtable {
	struct bucket_table __rcu	*tbl;
	unsigned int			key_len;
	unsigned int			max_elems;
	struct rhashtable_params	p;
	bool				rhlist;
	struct work_struct		run_work;
	struct mutex                    mutex;
	spinlock_t			lock;
	atomic_t			nelems;
};

struct rhashtable_params {
	u16			nelem_hint;
	u16			key_len;
	u16			key_offset;
	u16			head_offset;
	unsigned int		max_size;
	u16			min_size;
	bool			automatic_shrinking;
	rht_hashfn_t		hashfn;								<--- here
	rht_obj_hashfn_t	obj_hashfn;
	rht_obj_cmpfn_t		obj_cmpfn;
};
```

After inspecting the structure of `fl_flow_mask`, we find a function pointer `hashfn`, which serves as our function pointer to bypass KASLR! In my exploitation, it is assigned the `rhashtable_jhash2` function.

### off-by-one translates to control flow hijacking

Indeed, the situation is quite clear now.

```c
struct fl_flow_tmplt {
	struct fl_flow_key dummy_key;
	struct fl_flow_key mask;
	struct flow_dissector dissector;
	struct tcf_chain *chain;		<--- [24]
};

struct tcf_chain {
	/* Protects filter_chain. */
	struct mutex filter_chain_lock;
	struct tcf_proto __rcu *filter_chain;
	struct list_head list;
	struct tcf_block *block;		<--- [25]
	u32 index; /* chain index */
	unsigned int refcnt;
	unsigned int action_refcnt;
	bool explicitly_created;
	bool flushing;
	const struct tcf_proto_ops *tmplt_ops;
	void *tmplt_priv;
	struct rcu_head rcu;
};

struct tcf_block {
	/* Lock protects tcf_block and lifetime-management data of chains
	 * attached to the block (refcnt, action_refcnt, explicitly_created).
	 */
	struct mutex lock;
	struct list_head chain_list;
	u32 index; /* block index for shared blocks */
	u32 classid; /* which class this block belongs to */
	refcount_t refcnt;
	struct net *net;
	struct Qdisc *q;
	struct rw_semaphore cb_lock; /* protects cb_list and offload counters */
	struct flow_block flow_block;	<--- [26]
...
}

struct flow_block_cb {
	struct list_head	driver_list;
	struct list_head	list;
	flow_setup_cb_t		*cb;		<--- [27]
	void			*cb_ident;
	void			*cb_priv;
	void			(*release)(void *cb_priv);
	struct flow_block_indr	indr;
	unsigned int		refcnt;
};
```

We need to forge from [24] to [27], which requires full control over our known heap address. Here, I utilize the fuse+setxattr technique for heap spraying, allowing further control over the heap contents. For more information about this technique, refer to the provided links [2] and [4].

```c
static int
__tc_setup_cb_call(struct tcf_block *block, enum tc_setup_type type,
		   void *type_data, bool err_stop)
{
	struct flow_block_cb *block_cb;
	int ok_count = 0;
	int err;

	list_for_each_entry(block_cb, &block->flow_block.cb_list, list) {
		err = block_cb->cb(type, type_data, block_cb->cb_priv);
		if (err) {
			if (err_stop)
				return err;
		} else {
			ok_count++;
		}
	}
	return ok_count;
}
```

It's worth mentioning that, since we can forge the `cb` virtual function and control the third parameter `block_cb->cb_priv`, there are two potential privilege escalation strategies: one is direct ROP (Return-Oriented Programming), and the other is to find a suitable function to hijack control flow and convert it into arbitrary address read/write.

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/5.png">
</div>

As shown in the diagram above, we control four registers: `rdx`, `rcx` (utilized through multiple loops), `rax`, and `rip`. By using appropriate ROP gadgets, executing `commit_cred(prepare_kernel_cred(0))` can accomplish privilege escalation.

The technique used to return to user mode is `swapgs_restore_regs_and_return_to_usermode`. For more information on this technique, please refer to the provided link [3].

However, ROP (Return-Oriented Programming) is generally not very stable and requires extensive modifications to adapt to different kernel versions. Therefore, in the final version, I found a suitable function, `dbg_set_reg`, and `dbg_get_reg` (these two functions are available by default in Ubuntu), and combined them to modify `modprobe_path` to achieve privilege escalation.

## 0x03 The End

Thank you for reading. If you have any questions, feel free to contact [me](https://twitter.com/HBh25Y) or [Dawuge](https://twitter.com/Dawuge3).

<div class="text-center" style="margin-bottom: 1em">
<img src="/assets/images/2024-04-15/meme3.jpeg">
</div>

## 0x04 Reference

[cve-2021-22555](https://google.github.io/security-research/pocs/linux/cve-2021-22555/writeup.html)
[cve-2022-0185](https://www.willsroot.io/2022/01/cve-2022-0185.html)
[KPTI_bypass](https://www.anquanke.com/post/id/240006)
[Exploiting_race_conditions_on_ancient_Linux](https://static.sched.com/hosted_files/lsseu2019/04/LSSEU2019%20-%20Exploiting%20race%20conditions%20on%20Linux.pdf)
[commit](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4d56304e5827c8cc8cc18c75343d283af7c4825c)